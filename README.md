 Attack Category Flow

This repository contains a suite of tools for evaluating, analyzing, and visualizing classification outputs for multi-label attack incident classification models. It is designed specifically for outputs generated by **Gemini-1.5** and **Gemini-2.0** models.

---

##  Purpose

The toolkit supports:
- Computing key evaluation metrics.
- Visualizing classification performance.
- Summarizing missed, extra, and correctly predicted labels.
- Preparing outputs for systematic debugging and analysis.

---

##  Components

### 1. `metrics.py`

Computes multi-label classification metrics including:

- **Micro Precision**
- **Micro Recall**
- **Micro F1 Score**
- **Macro Precision**
- **Macro Recall**
- **Macro F1 Score**
- **Hamming Loss**

**How It Works:**

- Loads a `.json` file containing `true_labels` and `predicted_labels`.
- Uses a pre-defined label set (`LABELS`) for consistent binarization.
- Converts predictions and ground truths to binary arrays.
- Uses scikit-learn to calculate metrics.

**Usage:**

```bash
python metrics.py
